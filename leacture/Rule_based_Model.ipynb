{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hM1Q0gTWxe41",
        "outputId": "d35e0736-952d-465c-918b-0ae3c097a853"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Installing pandas...\n",
            "Installing numpy...\n",
            "Installing PyPDF2...\n",
            "Installing yfinance...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /content/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /content/nltk_data...\n",
            "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /content/nltk_data...\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Step 1: Auto-install missing packages\n",
        "import importlib\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "def install_if_missing(package_name, import_name=None):\n",
        "    try:\n",
        "        importlib.import_module(import_name if import_name else package_name)\n",
        "    except ImportError:\n",
        "        print(f\"Installing {package_name}...\")\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package_name])\n",
        "\n",
        "# List of required packages\n",
        "required_packages = [\n",
        "    (\"pandas\", \"pd\"),\n",
        "    (\"numpy\", \"np\"),\n",
        "    (\"PyPDF2\", \"PyPDF2\"),\n",
        "    (\"nltk\", \"nltk\"),\n",
        "    (\"scikit-learn\", \"sklearn\"),\n",
        "    (\"yfinance\", \"yfinance\"),\n",
        "    (\"matplotlib\", \"matplotlib\")\n",
        "]\n",
        "\n",
        "for pkg, imp in required_packages:\n",
        "    install_if_missing(pkg, imp)\n",
        "\n",
        "# Step 2: Import necessary libraries\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import PyPDF2\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import yfinance as yf\n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import datetime, timedelta\n",
        "import pickle\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Step 3: Clean and re-download NLTK resources\n",
        "import shutil\n",
        "\n",
        "# Fully remove nltk_data if it exists (forces a clean download)\n",
        "nltk_data_path = '/content/nltk_data'\n",
        "shutil.rmtree(nltk_data_path, ignore_errors=True)\n",
        "os.makedirs(nltk_data_path, exist_ok=True)\n",
        "\n",
        "# Set NLTK to look here first\n",
        "nltk.data.path.clear()\n",
        "nltk.data.path.append(nltk_data_path)\n",
        "\n",
        "# Download required corpora\n",
        "nltk.download('punkt', download_dir=nltk_data_path)\n",
        "nltk.download('stopwords', download_dir=nltk_data_path)\n",
        "nltk.download('wordnet', download_dir=nltk_data_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 249
        },
        "id": "T12piaO_xtQ2",
        "outputId": "f0df79ad-3824-4573-8084-82253ffb086f"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'google.colab'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolab\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m files\n\u001b[0;32m      2\u001b[0m uploaded \u001b[38;5;241m=\u001b[39m files\u001b[38;5;241m.\u001b[39mupload()\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "99gsPb0Tx5ST"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "# Create necessary directories\n",
        "os.makedirs('data/books', exist_ok=True)\n",
        "os.makedirs('data/processed', exist_ok=True)\n",
        "os.makedirs('data/knowledge_base', exist_ok=True)\n",
        "os.makedirs('data/models', exist_ok=True)\n",
        "os.makedirs('data/results', exist_ok=True)\n",
        "\n",
        "# Move all uploaded PDFs to data/books/\n",
        "for filename in uploaded.keys():\n",
        "    if filename.lower().endswith(\".pdf\"):\n",
        "        shutil.move(filename, f\"data/books/{filename}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D0g3XnJr56wK",
        "outputId": "85e62b12-6fa0-4440-838b-16d99409b47a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " little-book-that-still-beats-the-market-the-joel-greenblatt.pdf\n",
            "'security-analysis-benjamin-graham-6th-edition-pdf-february-24-2010-12-08-am-3-0-meg (1).pdf'\n",
            "'the-intelligent-investor (1).pdf'\n",
            "'TJA-Trading-In-The-Zone-master-the-market-with-confidence-discipline-and-a-winning-attitude-by-Mark-Douglas-Book-Novel-by-www.indianpdf.com_-Download-PDF-Online-Free (1).pdf'\n",
            " what-works-on-wall-street.pdf\n"
          ]
        }
      ],
      "source": [
        "!ls data/books\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TmAC9IHz6aFZ",
        "outputId": "6eee0bd8-ea89-4a14-f417-e1bb058568a3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: PyPDF2 in c:\\users\\pc\\appdata\\roaming\\python\\python312\\site-packages (3.0.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install PyPDF2\n",
        "import os, re, PyPDF2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "bBzyfCdx4lv9"
      },
      "outputs": [],
      "source": [
        "# Extract text from each PDF book\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    \"\"\"Extract text from a PDF file.\"\"\"\n",
        "    text = \"\"\n",
        "    try:\n",
        "        with open(pdf_path, 'rb') as file:\n",
        "            pdf_reader = PyPDF2.PdfReader(file)\n",
        "            for page_num in range(len(pdf_reader.pages)):\n",
        "                page = pdf_reader.pages[page_num]\n",
        "                text += page.extract_text()\n",
        "        return text\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting text from {pdf_path}: {str(e)}\")\n",
        "        return \"\"\n",
        "\n",
        "# Clean and preprocess text\n",
        "def clean_text(text):\n",
        "    \"\"\"Clean and preprocess the extracted text.\"\"\"\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove special characters and numbers (but keep periods and other sentence-ending punctuation)\n",
        "    text = re.sub(r'[^\\w\\s.,;:!?()\\'\\\"-]', ' ', text)\n",
        "\n",
        "    # Replace multiple spaces with a single space\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "\n",
        "    return text.strip()\n",
        "\n",
        "# Process all PDF books\n",
        "def process_all_books():\n",
        "    \"\"\"Process all PDF books in the data/books directory.\"\"\"\n",
        "    print(\"Processing PDF books...\")\n",
        "    books_data = {}\n",
        "\n",
        "    for filename in os.listdir('data/books'):\n",
        "        if filename.lower().endswith('.pdf'):\n",
        "            book_name = os.path.splitext(filename)[0]\n",
        "            file_path = os.path.join('data/books', filename)\n",
        "            print(f\"Extracting text from {book_name}...\")\n",
        "\n",
        "            book_text = extract_text_from_pdf(file_path)\n",
        "\n",
        "            if book_text:\n",
        "                # Save raw text\n",
        "                with open(f\"data/processed/{book_name}_raw.txt\", 'w', encoding='utf-8') as f:\n",
        "                    f.write(book_text)\n",
        "\n",
        "                # Clean text\n",
        "                book_text = clean_text(book_text)\n",
        "                books_data[book_name] = book_text\n",
        "\n",
        "                # Save cleaned text\n",
        "                with open(f\"data/processed/{book_name}_cleaned.txt\", 'w', encoding='utf-8') as f:\n",
        "                    f.write(book_text)\n",
        "\n",
        "                print(f\"Successfully processed {book_name} ({len(book_text)} characters)\")\n",
        "            else:\n",
        "                print(f\"Failed to extract text from {book_name}\")\n",
        "\n",
        "    return books_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Cl0HK6DAYVkA",
        "outputId": "deedb1e4-7ca6-42f8-e304-1deb708f47bf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing PDF books...\n",
            "Extracting text from TJA-Trading-In-The-Zone-master-the-market-with-confidence-discipline-and-a-winning-attitude-by-Mark-Douglas-Book-Novel-by-www.indianpdf.com_-Download-PDF-Online-Free (1)...\n",
            "Successfully processed TJA-Trading-In-The-Zone-master-the-market-with-confidence-discipline-and-a-winning-attitude-by-Mark-Douglas-Book-Novel-by-www.indianpdf.com_-Download-PDF-Online-Free (1) (436374 characters)\n",
            "Extracting text from security-analysis-benjamin-graham-6th-edition-pdf-february-24-2010-12-08-am-3-0-meg (1)...\n",
            "Successfully processed security-analysis-benjamin-graham-6th-edition-pdf-february-24-2010-12-08-am-3-0-meg (1) (1706461 characters)\n",
            "Extracting text from the-intelligent-investor (1)...\n",
            "Successfully processed the-intelligent-investor (1) (1276740 characters)\n",
            "Extracting text from little-book-that-still-beats-the-market-the-joel-greenblatt...\n",
            "Successfully processed little-book-that-still-beats-the-market-the-joel-greenblatt (209046 characters)\n",
            "Extracting text from what-works-on-wall-street...\n",
            "Successfully processed what-works-on-wall-street (210661 characters)\n"
          ]
        }
      ],
      "source": [
        "processed_books = process_all_books()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "z0HYxLlI46vu"
      },
      "outputs": [],
      "source": [
        "# Extract investment principles and rules\n",
        "def extract_investment_principles(books_data):\n",
        "    \"\"\"Extract investment principles and rules from book texts.\"\"\"\n",
        "    print(\"\\nExtracting investment principles...\")\n",
        "\n",
        "    # Keywords related to investment principles\n",
        "    rule_indicators = [\n",
        "        'rule', 'principle', 'strategy', 'method', 'approach', 'technique',\n",
        "        'important', 'essential', 'crucial', 'critical', 'key', 'fundamental',\n",
        "        'always', 'never', 'must', 'should', 'recommend', 'suggest',\n",
        "        'buy when', 'sell when', 'invest in', 'avoid', 'consider',\n",
        "        'indicator', 'signal', 'pattern', 'trend', 'analysis', 'valuation',\n",
        "        'price-to-earnings', 'p/e', 'dividend', 'yield', 'growth', 'value',\n",
        "        'bullish', 'bearish', 'market', 'stock', 'share', 'investment'\n",
        "    ]\n",
        "\n",
        "    all_principles = {}\n",
        "\n",
        "    for book_name, text in books_data.items():\n",
        "        print(f\"Extracting principles from {book_name}...\")\n",
        "        book_principles = []\n",
        "\n",
        "        # Split text into sentences\n",
        "        sentences = sent_tokenize(text)\n",
        "\n",
        "        # For each sentence, check if it contains rule indicators\n",
        "        for sentence in sentences:\n",
        "            sentence = sentence.strip()\n",
        "\n",
        "            # Skip short sentences\n",
        "            if len(sentence) < 25:\n",
        "                continue\n",
        "\n",
        "            # Check if sentence contains rule indicators\n",
        "            if any(indicator in sentence.lower() for indicator in rule_indicators):\n",
        "                # Additional filtering to ensure quality\n",
        "                # Make sure sentence is complete\n",
        "                if sentence[-1] in ['.', '!', '?', ':', ';']:\n",
        "                    book_principles.append(sentence)\n",
        "\n",
        "        # Remove duplicates and near-duplicates\n",
        "        book_principles = remove_duplicates(book_principles)\n",
        "\n",
        "        # Store principles\n",
        "        all_principles[book_name] = book_principles\n",
        "\n",
        "        # Save principles to file\n",
        "        with open(f\"data/knowledge_base/{book_name}_principles.txt\", 'w', encoding='utf-8') as f:\n",
        "            for principle in book_principles:\n",
        "                f.write(principle + \"\\n\\n\")\n",
        "\n",
        "        print(f\"Extracted {len(book_principles)} principles from {book_name}\")\n",
        "\n",
        "    return all_principles\n",
        "\n",
        "# Remove duplicate or very similar principles\n",
        "def remove_duplicates(principles):\n",
        "    \"\"\"Remove duplicate or very similar principles.\"\"\"\n",
        "    unique_principles = []\n",
        "\n",
        "    if not principles:\n",
        "        return unique_principles\n",
        "\n",
        "    # Create TF-IDF vectorizer\n",
        "    vectorizer = TfidfVectorizer()\n",
        "\n",
        "    for i, principle in enumerate(principles):\n",
        "        if i == 0:\n",
        "            unique_principles.append(principle)\n",
        "            continue\n",
        "\n",
        "        # Calculate similarity with existing unique principles\n",
        "        tfidf_matrix = vectorizer.fit_transform([principle] + unique_principles)\n",
        "        similarity_matrix = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:])\n",
        "\n",
        "        # If similarity is below threshold, add to unique principles\n",
        "        if not any(sim > 0.7 for sim in similarity_matrix[0]):\n",
        "            unique_principles.append(principle)\n",
        "\n",
        "    return unique_principles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CSWqGTUOKRjN",
        "outputId": "0d46223c-f970-4653-ad06-1491ceeee0c2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /content/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Manually load and patch the PunktSentenceTokenizer to avoid the phantom \"punkt_tab\" error\n",
        "import nltk.tokenize.punkt\n",
        "tokenizer_path = os.path.join(nltk_data_path, 'tokenizers/punkt/english.pickle')\n",
        "\n",
        "try:\n",
        "    with open(tokenizer_path, 'rb') as f:\n",
        "        punkt_tokenizer = nltk.tokenize.punkt.PunktSentenceTokenizer(pickle.load(f))\n",
        "        nltk.tokenize.sent_tokenize = punkt_tokenizer.tokenize\n",
        "except Exception as e:\n",
        "    print(f\"Manual punkt patch failed: {e}\")\n",
        "\n",
        "# Download 'punkt_tab' resource explicitly\n",
        "nltk.download('punkt_tab', download_dir=nltk_data_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BNLhSaU2HK2p",
        "outputId": "5081d5ab-4a18-4ee7-a783-bc6dcafbe8ce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Extracting investment principles...\n",
            "Extracting principles from TJA-Trading-In-The-Zone-master-the-market-with-confidence-discipline-and-a-winning-attitude-by-Mark-Douglas-Book-Novel-by-www.indianpdf.com_-Download-PDF-Online-Free (1)...\n",
            "Extracted 961 principles from TJA-Trading-In-The-Zone-master-the-market-with-confidence-discipline-and-a-winning-attitude-by-Mark-Douglas-Book-Novel-by-www.indianpdf.com_-Download-PDF-Online-Free (1)\n",
            "Extracting principles from security-analysis-benjamin-graham-6th-edition-pdf-february-24-2010-12-08-am-3-0-meg (1)...\n",
            "Extracted 5807 principles from security-analysis-benjamin-graham-6th-edition-pdf-february-24-2010-12-08-am-3-0-meg (1)\n",
            "Extracting principles from the-intelligent-investor (1)...\n",
            "Extracted 4202 principles from the-intelligent-investor (1)\n",
            "Extracting principles from little-book-that-still-beats-the-market-the-joel-greenblatt...\n",
            "Extracted 666 principles from little-book-that-still-beats-the-market-the-joel-greenblatt\n",
            "Extracting principles from what-works-on-wall-street...\n",
            "Extracted 1118 principles from what-works-on-wall-street\n"
          ]
        }
      ],
      "source": [
        "investment_principles = extract_investment_principles(processed_books)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "P9fvsP8W5Ex8"
      },
      "outputs": [],
      "source": [
        "# Categorize principles into relevant groups\n",
        "def categorize_principles(all_principles):\n",
        "    \"\"\"Categorize principles into relevant groups for stock analysis.\"\"\"\n",
        "    print(\"\\nCategorizing principles...\")\n",
        "\n",
        "    categories = {\n",
        "        'value_investing': ['value', 'intrinsic', 'margin of safety', 'undervalued', 'overvalued', 'book value', 'p/e', 'price-to-earnings', 'price to earnings'],\n",
        "        'growth_investing': ['growth', 'earnings growth', 'revenue growth', 'expanding', 'scalable', 'future potential'],\n",
        "        'technical_analysis': ['chart', 'pattern', 'trend', 'moving average', 'resistance', 'support', 'volume', 'momentum', 'oscillator'],\n",
        "        'fundamental_analysis': ['fundamental', 'balance sheet', 'income statement', 'cash flow', 'earnings', 'revenue', 'profit', 'margin'],\n",
        "        'risk_management': ['risk', 'diversification', 'allocation', 'portfolio', 'loss', 'hedge', 'protection', 'downside'],\n",
        "        'market_timing': ['timing', 'entry', 'exit', 'buy signal', 'sell signal', 'overbought', 'oversold', 'market condition'],\n",
        "        'psychological_factors': ['psychology', 'emotion', 'fear', 'greed', 'discipline', 'patience', 'confidence', 'contrarian'],\n",
        "        'company_quality': ['management', 'competitive advantage', 'moat', 'leadership', 'industry position', 'innovation', 'brand', 'market share']\n",
        "    }\n",
        "\n",
        "    categorized_principles = {category: [] for category in categories}\n",
        "\n",
        "    # Process each book's principles\n",
        "    for book_name, principles in all_principles.items():\n",
        "        for principle in principles:\n",
        "            # Assign to categories based on keywords\n",
        "            for category, keywords in categories.items():\n",
        "                if any(keyword in principle.lower() for keyword in keywords):\n",
        "                    # Add book source to principle\n",
        "                    categorized_principles[category].append({\n",
        "                        'principle': principle,\n",
        "                        'source': book_name\n",
        "                    })\n",
        "\n",
        "    # Save categorized principles\n",
        "    for category, principles in categorized_principles.items():\n",
        "        if principles:\n",
        "            with open(f\"data/knowledge_base/{category}_principles.txt\", 'w', encoding='utf-8') as f:\n",
        "                for principle_data in principles:\n",
        "                    f.write(f\"SOURCE: {principle_data['source']}\\n\")\n",
        "                    f.write(f\"PRINCIPLE: {principle_data['principle']}\\n\\n\")\n",
        "\n",
        "    # Create a unified knowledge base\n",
        "    create_unified_knowledge_base(categorized_principles)\n",
        "\n",
        "    return categorized_principles\n",
        "\n",
        "# Create a unified knowledge base\n",
        "def create_unified_knowledge_base(categorized_principles):\n",
        "    \"\"\"Create a unified knowledge base from all categorized principles.\"\"\"\n",
        "    unified_kb = []\n",
        "\n",
        "    for category, principles in categorized_principles.items():\n",
        "        for principle_data in principles:\n",
        "            unified_kb.append({\n",
        "                'category': category,\n",
        "                'principle': principle_data['principle'],\n",
        "                'source': principle_data['source']\n",
        "            })\n",
        "\n",
        "    # Save unified knowledge base\n",
        "    with open(\"data/knowledge_base/unified_knowledge_base.pkl\", 'wb') as f:\n",
        "        pickle.dump(unified_kb, f)\n",
        "\n",
        "    # Also save as text\n",
        "    with open(\"data/knowledge_base/unified_knowledge_base.txt\", 'w', encoding='utf-8') as f:\n",
        "        for entry in unified_kb:\n",
        "            f.write(f\"CATEGORY: {entry['category']}\\n\")\n",
        "            f.write(f\"SOURCE: {entry['source']}\\n\")\n",
        "            f.write(f\"PRINCIPLE: {entry['principle']}\\n\\n\")\n",
        "\n",
        "    print(f\"Created unified knowledge base with {len(unified_kb)} principles\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AnKu9rdANpop",
        "outputId": "9273e47c-31ae-43f4-c253-b8ec709e3d6d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Categorizing principles...\n",
            "Created unified knowledge base with 7046 principles\n"
          ]
        }
      ],
      "source": [
        "categorized_principles = categorize_principles(investment_principles)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "-yq2VrjQ6p7m"
      },
      "outputs": [],
      "source": [
        "# Load the knowledge base\n",
        "def load_knowledge_base():\n",
        "    \"\"\"Load the unified knowledge base.\"\"\"\n",
        "    try:\n",
        "        with open(\"data/knowledge_base/unified_knowledge_base.pkl\", 'rb') as f:\n",
        "            return pickle.load(f)\n",
        "    except FileNotFoundError:\n",
        "        print(\"Knowledge base not found. Please run the extraction process first.\")\n",
        "        return []\n",
        "\n",
        "import yfinance as yf\n",
        "import pandas as pd\n",
        "\n",
        "# Example: Get stock data for a ticker (e.g., Apple 'AAPL')\n",
        "#ticker = 'AAPL'\n",
        "#ticker_data = yf.download(ticker, period='30d', interval='1d')  # Last 30 days of data\n",
        "\n",
        "def calculate_financial_metrics(ticker_data, period=7):\n",
        "    \"\"\"Calculate key financial metrics for a stock over the given period (in days).\"\"\"\n",
        "    # Make sure we have enough data\n",
        "    if len(ticker_data) < period:\n",
        "        print(f\"Not enough data points. Need at least {period} days.\")\n",
        "        return None\n",
        "\n",
        "    # Get the most recent 'period' days of data\n",
        "    recent_data = ticker_data.tail(period)\n",
        "\n",
        "    # Debugging: Print the type and structure of 'recent_data' and 'recent_data['Close']'\n",
        "    print(\"Recent Data (last 7 days):\")\n",
        "    print(recent_data)\n",
        "\n",
        "    # Ensure 'Close' is treated as a Series\n",
        "    close_data = recent_data['Close']\n",
        "    print(\"Type of close_data:\", type(close_data))\n",
        "    print(\"Is close_data empty?\", close_data.empty)\n",
        "\n",
        "    if 'Close' not in recent_data.columns:\n",
        "        print(\"Error: 'Close' column is missing from the data.\")\n",
        "        return None\n",
        "\n",
        "     # Check if 'Close' data contains NaN values\n",
        "    if close_data.isnull().any().any():  # <--- Change here\n",
        "        print(\"Warning: 'Close' data contains NaN values. Please clean the data.\")\n",
        "        return None\n",
        "\n",
        "    metrics = {}\n",
        "\n",
        "    # Price metrics\n",
        "    closing_price = close_data.iloc[-1]  # Get the last row's closing price\n",
        "    metrics['closing_price'] = closing_price\n",
        "    metrics['avg_price'] = close_data.mean()\n",
        "    metrics['price_change'] = closing_price - close_data.iloc[0]  # Get the first row's closing price\n",
        "    metrics['price_change_pct'] = (metrics['price_change'] / close_data.iloc[0]) * 100\n",
        "\n",
        "    # Volume metrics\n",
        "    metrics['volume'] = recent_data['Volume'].iloc[-1]  # Get the last row's volume\n",
        "    metrics['avg_volume'] = recent_data['Volume'].mean()\n",
        "\n",
        "    # Volume change percentage\n",
        "    # Get the last element of the avg_volume Series to use for comparison\n",
        "    avg_volume_value = metrics['avg_volume']\n",
        "    # If the avg_volume Series has more than one value, use the last one\n",
        "    if isinstance(avg_volume_value, pd.Series):\n",
        "        avg_volume_value = avg_volume_value.iloc[-1]\n",
        "    metrics['volume_change_pct'] = (\n",
        "        (metrics['volume'] - metrics['avg_volume']) / metrics['avg_volume']) * 100 if avg_volume_value > 0 else 0\n",
        "\n",
        "\n",
        "    # Trend metrics (ensure correct scalar comparison)\n",
        "    metrics['uptrend'] = close_data.iloc[-1] > close_data.iloc[0]  # Compare scalars (individual values)\n",
        "\n",
        "    # Volatility metrics\n",
        "    metrics['volatility'] = close_data.std()\n",
        "    metrics['volatility_pct'] = (metrics['volatility'] / metrics['avg_price']) * 100\n",
        "\n",
        "    # Technical indicators (Simple Moving Average)\n",
        "    metrics['sma_5'] = close_data.rolling(window=min(5, period)).mean().iloc[-1]\n",
        "\n",
        "    # Check for enough data to calculate 20-day SMA\n",
        "    if len(ticker_data) >= 20:\n",
        "        metrics['sma_20'] = ticker_data['Close'].rolling(window=20).mean().iloc[-1]\n",
        "        # Moving average crossover signal\n",
        "        metrics['ma_crossover_signal'] = metrics['sma_5'] > metrics['sma_20']\n",
        "    else:\n",
        "        metrics['sma_20'] = None\n",
        "        metrics['ma_crossover_signal'] = None\n",
        "\n",
        "    return metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ByVtvSqpN3J1",
        "outputId": "3eff8a0b-e7a1-4a81-9231-1cac713bfb64"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded knowledge base with 7046 principles.\n"
          ]
        }
      ],
      "source": [
        "# Load the knowledge base after extraction\n",
        "knowledge_base = load_knowledge_base()\n",
        "\n",
        "# Check if knowledge base was successfully loaded\n",
        "if knowledge_base:\n",
        "    print(f\"Loaded knowledge base with {len(knowledge_base)} principles.\")\n",
        "else:\n",
        "    print(\"No knowledge base found. Please extract principles first.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kYlDPenjN9zQ",
        "outputId": "f22e07d7-3eb2-4545-f2df-640fcedda075"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r[*********************100%***********************]  1 of 1 completed\n",
            "ERROR:yfinance:\n",
            "1 Failed download:\n",
            "ERROR:yfinance:['AAPL']: YFRateLimitError('Too Many Requests. Rate limited. Try after a while.')\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Not enough data points. Need at least 7 days.\n",
            "Not enough data to calculate metrics for AAPL.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "# Example: Get stock data for a ticker (e.g., Apple 'AAPL')\n",
        "ticker = 'AAPL'\n",
        "ticker_data = yf.download(ticker, period='30d', interval='1d')  # Last 30 days of data\n",
        "\n",
        "# Calculate financial metrics over the last 7 days\n",
        "metrics = calculate_financial_metrics(ticker_data, period=7)\n",
        "\n",
        "# Display the calculated metrics\n",
        "if metrics:\n",
        "    print(f\"Calculated metrics for {ticker}:\")\n",
        "    print(metrics)\n",
        "else:\n",
        "    print(f\"Not enough data to calculate metrics for {ticker}.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "-fYo3lLT6wzA"
      },
      "outputs": [],
      "source": [
        "# Function to analyze stock based on knowledge base principles\n",
        "def analyze_stock_with_principles(ticker_symbol, knowledge_base, period=7):\n",
        "    \"\"\"Analyze a stock using principles from the knowledge base.\"\"\"\n",
        "    print(f\"\\nAnalyzing {ticker_symbol} using knowledge-based principles...\")\n",
        "\n",
        "    try:\n",
        "        # Get stock data for analysis\n",
        "        end_date = datetime.now()\n",
        "        start_date = end_date - timedelta(days=period * 2)  # Get more data than needed for calculations\n",
        "\n",
        "        ticker_data = yf.download(ticker_symbol, start=start_date, end=end_date)\n",
        "\n",
        "        if ticker_data.empty:\n",
        "            print(f\"No data found for {ticker_symbol}\")\n",
        "            return None\n",
        "\n",
        "        # Calculate financial metrics\n",
        "        metrics = calculate_financial_metrics(ticker_data, period)\n",
        "\n",
        "        if not metrics:\n",
        "            return None\n",
        "\n",
        "        # Analyze based on principles in the knowledge base\n",
        "        analysis_results = {\n",
        "            'ticker': ticker_symbol,\n",
        "            'analysis_date': datetime.now().strftime(\"%Y-%m-%d\"),\n",
        "            'metrics': metrics,\n",
        "            'insights': [],\n",
        "            'signals': {'buy': 0, 'hold': 0, 'sell': 0}\n",
        "        }\n",
        "\n",
        "        # Apply principles from each category\n",
        "        for entry in knowledge_base:\n",
        "            category = entry['category']\n",
        "            principle = entry['principle']\n",
        "\n",
        "            # Generate insights based on principles and metrics\n",
        "            if category == 'technical_analysis':\n",
        "                if 'trend' in principle.lower() and metrics['uptrend']:\n",
        "                    analysis_results['insights'].append({\n",
        "                        'category': category,\n",
        "                        'principle': \"Uptrend detected in recent price movement.\",\n",
        "                        'signal': 'buy'\n",
        "                    })\n",
        "                    analysis_results['signals']['buy'] += 1\n",
        "\n",
        "                if 'moving average' in principle.lower() and metrics['sma_20'] and metrics['ma_crossover_signal']:\n",
        "                    analysis_results['insights'].append({\n",
        "                        'category': category,\n",
        "                        'principle': \"Short-term moving average crossed above long-term moving average, suggesting potential upward momentum.\",\n",
        "                        'signal': 'buy'\n",
        "                    })\n",
        "                    analysis_results['signals']['buy'] += 1\n",
        "\n",
        "                if 'volume' in principle.lower() and metrics['volume_change_pct'] > 20:\n",
        "                    analysis_results['insights'].append({\n",
        "                        'category': category,\n",
        "                        'principle': \"Significant increase in trading volume, suggesting strong interest.\",\n",
        "                        'signal': 'buy' if metrics['price_change_pct'] > 0 else 'sell'\n",
        "                    })\n",
        "                    if metrics['price_change_pct'] > 0:\n",
        "                        analysis_results['signals']['buy'] += 1\n",
        "                    else:\n",
        "                        analysis_results['signals']['sell'] += 1\n",
        "\n",
        "            elif category == 'value_investing':\n",
        "                # We don't have P/E and other fundamental data in our simple model\n",
        "                # But we can look at price trends\n",
        "                if 'undervalued' in principle.lower() and metrics['price_change_pct'] < -5:\n",
        "                    analysis_results['insights'].append({\n",
        "                        'category': category,\n",
        "                        'principle': \"Recent price decline may indicate potential value opportunity if fundamentals are strong.\",\n",
        "                        'signal': 'buy'\n",
        "                    })\n",
        "                    analysis_results['signals']['buy'] += 1\n",
        "\n",
        "            elif category == 'risk_management':\n",
        "                if 'volatility' in principle.lower() and metrics['volatility_pct'] > 3:\n",
        "                    analysis_results['insights'].append({\n",
        "                        'category': category,\n",
        "                        'principle': \"High volatility detected, suggesting increased risk.\",\n",
        "                        'signal': 'hold'\n",
        "                    })\n",
        "                    analysis_results['signals']['hold'] += 1\n",
        "\n",
        "        # Generate additional insights based on metrics\n",
        "        if metrics['price_change_pct'] > 5:\n",
        "            analysis_results['insights'].append({\n",
        "                'category': 'price_momentum',\n",
        "                'principle': \"Strong positive price momentum in the analysis period.\",\n",
        "                'signal': 'buy'\n",
        "            })\n",
        "            analysis_results['signals']['buy'] += 1\n",
        "\n",
        "        if metrics['price_change_pct'] < -5:\n",
        "            analysis_results['insights'].append({\n",
        "                'category': 'price_momentum',\n",
        "                'principle': \"Significant price decline in the analysis period.\",\n",
        "                'signal': 'sell'\n",
        "            })\n",
        "            analysis_results['signals']['sell'] += 1\n",
        "\n",
        "        # Calculate probabilities\n",
        "        total_signals = sum(analysis_results['signals'].values())\n",
        "        if total_signals > 0:\n",
        "            analysis_results['probabilities'] = {\n",
        "                'up': analysis_results['signals']['buy'] / total_signals,\n",
        "                'neutral': analysis_results['signals']['hold'] / total_signals,\n",
        "                'down': analysis_results['signals']['sell'] / total_signals\n",
        "            }\n",
        "        else:\n",
        "            analysis_results['probabilities'] = {'up': 0.33, 'neutral': 0.34, 'down': 0.33}\n",
        "\n",
        "        # Generate recommendation\n",
        "        if analysis_results['probabilities']['up'] > 0.5:\n",
        "            analysis_results['recommendation'] = 'BUY'\n",
        "        elif analysis_results['probabilities']['down'] > 0.5:\n",
        "            analysis_results['recommendation'] = 'SELL'\n",
        "        else:\n",
        "            analysis_results['recommendation'] = 'HOLD'\n",
        "\n",
        "        # Create explanation\n",
        "        analysis_results['explanation'] = f\"Analysis of {ticker_symbol} based on investment principles suggests a {analysis_results['recommendation']} recommendation. \"\n",
        "        analysis_results['explanation'] += f\"The probability of price increase is {analysis_results['probabilities']['up']:.2%}, \"\n",
        "        analysis_results['explanation'] += f\"neutral is {analysis_results['probabilities']['neutral']:.2%}, \"\n",
        "        analysis_results['explanation'] += f\"and decrease is {analysis_results['probabilities']['down']:.2%}.\"\n",
        "\n",
        "        # Save results\n",
        "        with open(f\"data/results/{ticker_symbol}_analysis.pkl\", 'wb') as f:\n",
        "            pickle.dump(analysis_results, f)\n",
        "\n",
        "        print(f\"Analysis complete for {ticker_symbol}. Recommendation: {analysis_results['recommendation']}\")\n",
        "        return analysis_results\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error analyzing {ticker_symbol}: {str(e)}\")\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "53T7oiRxTNBE",
        "outputId": "5d990716-dffb-43ff-cc83-1345b4a4b25f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Analyzing AAPL using knowledge-based principles...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r[*********************100%***********************]  1 of 1 completed\n",
            "ERROR:yfinance:\n",
            "1 Failed download:\n",
            "ERROR:yfinance:['AAPL']: YFRateLimitError('Too Many Requests. Rate limited. Try after a while.')\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No data found for AAPL\n",
            "No analysis results for AAPL.\n"
          ]
        }
      ],
      "source": [
        "# Assuming the knowledge base is already loaded and available\n",
        "ticker_symbol = 'AAPL'  # Example ticker\n",
        "period = 7  # Analysis period (default is 7 days)\n",
        "\n",
        "# Call the function to analyze the stock based on knowledge base principles\n",
        "analysis_results = analyze_stock_with_principles(ticker_symbol, knowledge_base, period)\n",
        "\n",
        "# Print the analysis results\n",
        "if analysis_results:\n",
        "    print(f\"Analysis for {ticker_symbol}:\")\n",
        "    print(analysis_results)\n",
        "else:\n",
        "    print(f\"No analysis results for {ticker_symbol}.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "MYRdBzD761en"
      },
      "outputs": [],
      "source": [
        "# Function to display analysis results\n",
        "def display_analysis_results(analysis_results):\n",
        "    \"\"\"Display analysis results in a readable format.\"\"\"\n",
        "    if not analysis_results:\n",
        "        print(\"No analysis results to display.\")\n",
        "        return\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(f\"STOCK ANALYSIS: {analysis_results['ticker']}\")\n",
        "    print(f\"Date: {analysis_results['analysis_date']}\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    print(\"\\nMETRICS:\")\n",
        "    metrics = analysis_results['metrics']\n",
        "    print(f\"Current Price: ${metrics['closing_price']:.2f}\")\n",
        "    print(f\"Price Change (week): {metrics['price_change_pct']:.2f}%\")\n",
        "    print(f\"Average Volume: {metrics['avg_volume']:.0f}\")\n",
        "    print(f\"Volatility: {metrics['volatility_pct']:.2f}%\")\n",
        "\n",
        "    print(\"\\nINSIGHTS:\")\n",
        "    for i, insight in enumerate(analysis_results['insights'], 1):\n",
        "        print(f\"{i}. {insight['principle']} ({insight['category'].replace('_', ' ').title()}) - Signal: {insight['signal'].upper()}\")\n",
        "\n",
        "    print(\"\\nPROBABILITIES:\")\n",
        "    print(f\"Price Up: {analysis_results['probabilities']['up']:.2%}\")\n",
        "    print(f\"Price Neutral: {analysis_results['probabilities']['neutral']:.2%}\")\n",
        "    print(f\"Price Down: {analysis_results['probabilities']['down']:.2%}\")\n",
        "\n",
        "    print(\"\\nRECOMMENDATION:\")\n",
        "    print(analysis_results['recommendation'])\n",
        "\n",
        "    print(\"\\nEXPLANATION:\")\n",
        "    print(analysis_results['explanation'])\n",
        "    print(\"=\"*50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9IbV_6KoTUB0",
        "outputId": "ef81fba3-64b5-4c8b-a4a5-8cdbdea34628"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Analyzing AAPL using knowledge-based principles...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r[*********************100%***********************]  1 of 1 completed\n",
            "ERROR:yfinance:\n",
            "1 Failed download:\n",
            "ERROR:yfinance:['AAPL']: YFRateLimitError('Too Many Requests. Rate limited. Try after a while.')\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No data found for AAPL\n",
            "No analysis results to display.\n"
          ]
        }
      ],
      "source": [
        "# Assuming the analysis results are available from the analyze_stock_with_principles function\n",
        "ticker_symbol = 'AAPL'  # Example ticker\n",
        "period = 7  # Analysis period (default is 7 days)\n",
        "\n",
        "# First, analyze the stock based on knowledge base principles\n",
        "analysis_results = analyze_stock_with_principles(ticker_symbol, knowledge_base, period)\n",
        "\n",
        "# Now, display the analysis results\n",
        "display_analysis_results(analysis_results)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "Op8Paf3c65lt"
      },
      "outputs": [],
      "source": [
        "# Main function to run the model\n",
        "def run_rule_based_model():\n",
        "    \"\"\"Run the rule-based stock prediction model.\"\"\"\n",
        "    print(\"\\nRunning the Rule-Based Stock Prediction Model (Model 1)...\")\n",
        "\n",
        "    # Check if knowledge base exists\n",
        "    if not os.path.exists(\"data/knowledge_base/unified_knowledge_base.pkl\"):\n",
        "        # Process books and extract principles\n",
        "        books_data = process_all_books()\n",
        "        all_principles = extract_investment_principles(books_data)\n",
        "        categorized_principles = categorize_principles(all_principles)\n",
        "\n",
        "    # Load knowledge base\n",
        "    knowledge_base = load_knowledge_base()\n",
        "\n",
        "    if not knowledge_base:\n",
        "        print(\"Knowledge base is empty. Please check the extraction process.\")\n",
        "        return\n",
        "\n",
        "    print(f\"Loaded knowledge base with {len(knowledge_base)} principles.\")\n",
        "\n",
        "    # Define stocks to analyze\n",
        "    stocks_to_analyze = ['AAPL', 'MSFT', 'GOOGL', 'AMZN', 'META']\n",
        "\n",
        "    # Analyze each stock\n",
        "    analysis_results = {}\n",
        "    for ticker in stocks_to_analyze:\n",
        "        result = analyze_stock_with_principles(ticker, knowledge_base)\n",
        "        if result:\n",
        "            analysis_results[ticker] = result\n",
        "            display_analysis_results(result)\n",
        "\n",
        "    return analysis_results\n",
        "\n",
        "# Function to analyze a specific stock (for external usage)\n",
        "def analyze_specific_stock(ticker_symbol, period=7):\n",
        "    \"\"\"Analyze a specific stock using the rule-based model.\"\"\"\n",
        "    # Load knowledge base\n",
        "    knowledge_base = load_knowledge_base()\n",
        "\n",
        "    if not knowledge_base:\n",
        "        print(\"Knowledge base is empty. Please run the full model first.\")\n",
        "        return None\n",
        "\n",
        "    # Analyze the stock\n",
        "    result = analyze_stock_with_principles(ticker_symbol, knowledge_base, period)\n",
        "\n",
        "    # Display results\n",
        "    if result:\n",
        "        display_analysis_results(result)\n",
        "\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "phyZTrUyTixU",
        "outputId": "3d290860-8b5e-44ee-a851-cb564e45a102"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Running the Rule-Based Stock Prediction Model (Model 1)...\n",
            "Loaded knowledge base with 7046 principles.\n",
            "\n",
            "Analyzing AAPL using knowledge-based principles...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r[*********************100%***********************]  1 of 1 completed\n",
            "ERROR:yfinance:\n",
            "1 Failed download:\n",
            "ERROR:yfinance:['AAPL']: YFRateLimitError('Too Many Requests. Rate limited. Try after a while.')\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No data found for AAPL\n",
            "\n",
            "Analyzing MSFT using knowledge-based principles...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r[*********************100%***********************]  1 of 1 completed\n",
            "ERROR:yfinance:\n",
            "1 Failed download:\n",
            "ERROR:yfinance:['MSFT']: YFRateLimitError('Too Many Requests. Rate limited. Try after a while.')\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No data found for MSFT\n",
            "\n",
            "Analyzing GOOGL using knowledge-based principles...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r[*********************100%***********************]  1 of 1 completed\n",
            "ERROR:yfinance:\n",
            "1 Failed download:\n",
            "ERROR:yfinance:['GOOGL']: YFRateLimitError('Too Many Requests. Rate limited. Try after a while.')\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No data found for GOOGL\n",
            "\n",
            "Analyzing AMZN using knowledge-based principles...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r[*********************100%***********************]  1 of 1 completed\n",
            "ERROR:yfinance:\n",
            "1 Failed download:\n",
            "ERROR:yfinance:['AMZN']: YFRateLimitError('Too Many Requests. Rate limited. Try after a while.')\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No data found for AMZN\n",
            "\n",
            "Analyzing META using knowledge-based principles...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r[*********************100%***********************]  1 of 1 completed\n",
            "ERROR:yfinance:\n",
            "1 Failed download:\n",
            "ERROR:yfinance:['META']: YFRateLimitError('Too Many Requests. Rate limited. Try after a while.')\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No data found for META\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{}"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Call the run_rule_based_model to analyze a set of stocks\n",
        "run_rule_based_model()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xYQCPJAjToEh",
        "outputId": "55684024-1a19-44b4-d09c-bf844bde4d46"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Analyzing AAPL using knowledge-based principles...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r[*********************100%***********************]  1 of 1 completed\n",
            "ERROR:yfinance:\n",
            "1 Failed download:\n",
            "ERROR:yfinance:['AAPL']: YFRateLimitError('Too Many Requests. Rate limited. Try after a while.')\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No data found for AAPL\n"
          ]
        }
      ],
      "source": [
        "# Example of calling analyze_specific_stock for a single stock (e.g., 'AAPL')\n",
        "ticker_symbol = 'AAPL'  # Stock ticker to analyze\n",
        "period = 7  # Default analysis period is 7 days\n",
        "\n",
        "result = analyze_specific_stock(ticker_symbol, period)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bTgnI5Z16-q7"
      },
      "outputs": [],
      "source": [
        "# Example usage to analyze a stock report\n",
        "def analyze_stock_report(ticker_symbol, report_text=None, period=7):\n",
        "    \"\"\"\n",
        "    Analyze a stock report using the rule-based model.\n",
        "    If report_text is provided, it will be considered in the analysis.\n",
        "    Otherwise, only historical data will be used.\n",
        "    \"\"\"\n",
        "    # Load knowledge base\n",
        "    knowledge_base = load_knowledge_base()\n",
        "\n",
        "    if not knowledge_base:\n",
        "        print(\"Knowledge base is empty. Please run the full model first.\")\n",
        "        return None\n",
        "\n",
        "    # Analyze based on historical data\n",
        "    result = analyze_stock_with_principles(ticker_symbol, knowledge_base, period)\n",
        "\n",
        "    # If report text is provided, enhance analysis with it\n",
        "    if report_text and result:\n",
        "        # Analyze report for sentiment and key information\n",
        "        sentiment_score = 0\n",
        "\n",
        "        # Simple keyword-based sentiment analysis\n",
        "        positive_keywords = ['growth', 'profit', 'increase', 'exceed', 'beat', 'positive', 'strong', 'success', 'innovation']\n",
        "        negative_keywords = ['decline', 'loss', 'decrease', 'miss', 'below', 'negative', 'weak', 'failure', 'risk']\n",
        "\n",
        "        # Count occurrences\n",
        "        pos_count = sum(report_text.lower().count(word) for word in positive_keywords)\n",
        "        neg_count = sum(report_text.lower().count(word) for word in negative_keywords)\n",
        "\n",
        "        # Calculate sentiment score (-1 to 1)\n",
        "        total_count = pos_count + neg_count\n",
        "        if total_count > 0:\n",
        "            sentiment_score = (pos_count - neg_count) / total_count\n",
        "\n",
        "        # Add report analysis to results\n",
        "        result['report_analysis'] = {\n",
        "            'sentiment_score': sentiment_score,\n",
        "            'sentiment': 'Positive' if sentiment_score > 0.2 else ('Negative' if sentiment_score < -0.2 else 'Neutral')\n",
        "        }\n",
        "\n",
        "        # Adjust probabilities based on report sentiment\n",
        "        if sentiment_score > 0.3:\n",
        "            result['probabilities']['up'] = min(0.9, result['probabilities']['up'] + 0.2)\n",
        "            result['probabilities']['down'] = max(0.1, result['probabilities']['down'] - 0.2)\n",
        "        elif sentiment_score < -0.3:\n",
        "            result['probabilities']['down'] = min(0.9, result['probabilities']['down'] + 0.2)\n",
        "            result['probabilities']['up'] = max(0.1, result['probabilities']['up'] - 0.2)\n",
        "\n",
        "        # Normalize probabilities\n",
        "        total_prob = sum(result['probabilities'].values())\n",
        "        for key in result['probabilities']:\n",
        "            result['probabilities'][key] /= total_prob\n",
        "\n",
        "        # Update recommendation\n",
        "        if result['probabilities']['up'] > 0.5:\n",
        "            result['recommendation'] = 'BUY'\n",
        "        elif result['probabilities']['down'] > 0.5:\n",
        "            result['recommendation'] = 'SELL'\n",
        "        else:\n",
        "            result['recommendation'] = 'HOLD'\n",
        "\n",
        "        # Update explanation\n",
        "        result['explanation'] = f\"Analysis of {ticker_symbol} based on investment principles and recent report \"\n",
        "        result['explanation'] += f\"suggests a {result['recommendation']} recommendation. \"\n",
        "        result['explanation'] += f\"The report sentiment is {result['report_analysis']['sentiment']} ({result['report_analysis']['sentiment_score']:.2f}). \"\n",
        "        result['explanation'] += f\"The probability of price increase is {result['probabilities']['up']:.2%}, \"\n",
        "        result['explanation'] += f\"neutral is {result['probabilities']['neutral']:.2%}, \"\n",
        "        result['explanation'] += f\"and decrease is {result['probabilities']['down']:.2%}.\"\n",
        "\n",
        "    # Display results\n",
        "    if result:\n",
        "        display_analysis_results(result)\n",
        "\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YorYSYjgT0md",
        "outputId": "75fbed71-215b-4fc6-c184-86a2f67710f2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r[*********************100%***********************]  1 of 1 completed"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Analyzing AAPL using knowledge-based principles...\n",
            "Recent Data (last 7 days):\n",
            "Price            Close        High         Low        Open     Volume\n",
            "Ticker            AAPL        AAPL        AAPL        AAPL       AAPL\n",
            "Date                                                                 \n",
            "2025-04-14  202.520004  212.940002  201.160004  211.440002  101352900\n",
            "2025-04-15  202.139999  203.509995  199.800003  201.860001   51343900\n",
            "2025-04-16  194.270004  200.699997  192.369995  198.360001   59732400\n",
            "2025-04-17  196.979996  198.830002  194.419998  197.199997   51334300\n",
            "2025-04-21  193.160004  193.800003  189.809998  193.270004   46742500\n",
            "2025-04-22  199.740005  201.589996  195.970001  196.119995   52976400\n",
            "2025-04-23  204.600006  208.000000  202.798996  206.000000   51988230\n",
            "Type of close_data: <class 'pandas.core.frame.DataFrame'>\n",
            "Is close_data empty? False\n",
            "Error analyzing AAPL: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Analyze stock without a report (only using historical data)\n",
        "ticker_symbol = 'AAPL'  # Example stock ticker\n",
        "period = 7  # Default period (7 days)\n",
        "\n",
        "# Call the function\n",
        "result = analyze_stock_report(ticker_symbol, report_text=None, period=period)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uaQekZzj7E0u",
        "outputId": "14ca7aef-cbcf-44ac-beae-3970b82e4577"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r[*********************100%***********************]  1 of 1 completed"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Running the Rule-Based Stock Prediction Model (Model 1)...\n",
            "Loaded knowledge base with 5951 principles.\n",
            "\n",
            "Analyzing AAPL using knowledge-based principles...\n",
            "Recent Data (last 7 days):\n",
            "Price            Close        High         Low        Open     Volume\n",
            "Ticker            AAPL        AAPL        AAPL        AAPL       AAPL\n",
            "Date                                                                 \n",
            "2025-04-14  202.520004  212.940002  201.160004  211.440002  101352900\n",
            "2025-04-15  202.139999  203.509995  199.800003  201.860001   51343900\n",
            "2025-04-16  194.270004  200.699997  192.369995  198.360001   59732400\n",
            "2025-04-17  196.979996  198.830002  194.419998  197.199997   51334300\n",
            "2025-04-21  193.160004  193.800003  189.809998  193.270004   46742500\n",
            "2025-04-22  199.740005  201.589996  195.970001  196.119995   52976400\n",
            "2025-04-23  204.600006  208.000000  202.798996  206.000000   51988230\n",
            "Type of close_data: <class 'pandas.core.frame.DataFrame'>\n",
            "Is close_data empty? False\n",
            "Error analyzing AAPL: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().\n",
            "\n",
            "Analyzing MSFT using knowledge-based principles...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Recent Data (last 7 days):\n",
            "Price            Close        High         Low        Open    Volume\n",
            "Ticker            MSFT        MSFT        MSFT        MSFT      MSFT\n",
            "Date                                                                \n",
            "2025-04-14  387.809998  394.649994  384.209991  393.220001  19251200\n",
            "2025-04-15  385.730011  391.890015  384.160004  388.510010  17199900\n",
            "2025-04-16  371.609985  381.609985  368.000000  380.670013  21967800\n",
            "2025-04-17  367.779999  374.320007  366.890015  373.750000  20943700\n",
            "2025-04-21  359.119995  364.480011  355.670013  362.820007  20807300\n",
            "2025-04-22  366.820007  367.769989  359.859985  363.380005  19485000\n",
            "2025-04-23  374.390015  380.390015  373.029999  376.059998  20161345\n",
            "Type of close_data: <class 'pandas.core.frame.DataFrame'>\n",
            "Is close_data empty? False\n",
            "Error analyzing MSFT: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().\n",
            "\n",
            "Analyzing GOOGL using knowledge-based principles...\n",
            "Recent Data (last 7 days):\n",
            "Price            Close        High         Low        Open    Volume\n",
            "Ticker           GOOGL       GOOGL       GOOGL       GOOGL     GOOGL\n",
            "Date                                                                \n",
            "2025-04-14  159.070007  161.720001  157.559998  160.000000  30333000\n",
            "2025-04-15  156.309998  159.649994  155.210007  159.130005  27551500\n",
            "2025-04-16  153.330002  155.889999  151.509995  153.100006  28187400\n",
            "2025-04-17  151.160004  154.679993  148.500000  154.289993  32938500\n",
            "2025-04-21  147.669998  148.949997  146.100006  148.880005  26049100\n",
            "2025-04-22  151.470001  152.190002  148.539993  148.889999  26971800\n",
            "2025-04-23  155.350006  157.524994  153.809998  155.610001  30265543\n",
            "Type of close_data: <class 'pandas.core.frame.DataFrame'>\n",
            "Is close_data empty? False\n",
            "Error analyzing GOOGL: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().\n",
            "\n",
            "Analyzing AMZN using knowledge-based principles...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Recent Data (last 7 days):\n",
            "Price            Close        High         Low        Open    Volume\n",
            "Ticker            AMZN        AMZN        AMZN        AMZN      AMZN\n",
            "Date                                                                \n",
            "2025-04-14  182.119995  187.440002  179.229996  186.839996  48002500\n",
            "2025-04-15  179.589996  182.350006  177.929993  181.410004  43642000\n",
            "2025-04-16  174.330002  179.100006  171.410004  176.289993  51875300\n",
            "2025-04-17  172.610001  176.210007  172.000000  176.000000  44468400\n",
            "2025-04-21  167.320007  169.600006  165.289993  169.600006  48126100\n",
            "2025-04-22  173.179993  176.779999  169.350006  169.850006  56607200\n",
            "2025-04-23  180.600006  187.380005  180.199997  183.440002  61598390\n",
            "Type of close_data: <class 'pandas.core.frame.DataFrame'>\n",
            "Is close_data empty? False\n",
            "Error analyzing AMZN: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().\n",
            "\n",
            "Analyzing META using knowledge-based principles...\n",
            "Recent Data (last 7 days):\n",
            "Price            Close        High         Low        Open    Volume\n",
            "Ticker            META        META        META        META      META\n",
            "Date                                                                \n",
            "2025-04-14  531.479980  557.770020  528.280029  556.169983  14130900\n",
            "2025-04-15  521.520020  537.940002  517.500000  532.109985  15558700\n",
            "2025-04-16  502.309998  513.369995  495.630005  508.510010  18735100\n",
            "2025-04-17  501.480011  507.299988  498.010010  505.250000  14593500\n",
            "2025-04-21  484.660004  493.500000  479.799988  491.329987  16166000\n",
            "2025-04-22  500.279999  506.880005  486.359985  491.869995  17399400\n",
            "2025-04-23  520.270020  535.337585  516.534973  528.525024  17959170\n",
            "Type of close_data: <class 'pandas.core.frame.DataFrame'>\n",
            "Is close_data empty? False\n",
            "Error analyzing META: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().\n",
            "Model is ready to use! Uncomment the example usage above to test it.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# This cell demonstrates how to use the model\n",
        "# You can uncomment the appropriate part to run\n",
        "\n",
        "# Run the complete model to process books and analyze stocks\n",
        "analysis_results = run_rule_based_model()\n",
        "\n",
        "# OR analyze a specific stock\n",
        "# result = analyze_specific_stock('TSLA')\n",
        "\n",
        "# OR analyze a stock with a news report\n",
        "# example_report = \"\"\"\n",
        "# Tesla reported strong quarterly earnings that exceeded analyst expectations.\n",
        "# Revenue grew by 20% year-over-year, driven by increased vehicle deliveries and growth in the energy business.\n",
        "# The company also announced plans to expand production capacity in coming quarters.\n",
        "# \"\"\"\n",
        "# result = analyze_stock_report('TSLA', example_report)\n",
        "\n",
        "print(\"Model is ready to use! Uncomment the example usage above to test it.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X3PMk0Xo7LPw"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
